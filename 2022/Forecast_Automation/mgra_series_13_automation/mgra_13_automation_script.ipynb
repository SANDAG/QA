{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MGRA Series 13 Specific Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of runtime which will be printed at the end of this notebook\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy as sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDAM is the sql server where fact tables live\n",
    "# SQL2014B8 is required for the MGRA -> LUZ crosswalk\n",
    "DDAM = sql.create_engine('mssql+pymssql://DDAMWSQL16/')\n",
    "SQL2014B8 = sql.create_engine('mssql+pymssql://SQL2014B8/')\n",
    "\n",
    "# Where to save outputs\n",
    "# Raw outputs are the files downloaded directly from SQL server with minimal modification\n",
    "# Aggregated outputs are combined files (all fact tables) aggregated to specific geography levels\n",
    "# Both outputs are TODO @Calvin I don't know what these are tbh, can you fill them out\n",
    "# Diff outputs are TODO @Calvin same ^\n",
    "BASE_FOLDER = pathlib.Path(\"J:/DataScience/DataQuality/QAQC/forecast_automation/mgra_series_13_outputs/\")\n",
    "RAW_FOLDER = BASE_FOLDER / \"raw_data/\"\n",
    "AGGREGATED_FOLDER = BASE_FOLDER / \"aggregated_data/\"\n",
    "BOTH_FOLDER = BASE_FOLDER / \"both_files/\"\n",
    "DIFF_FOLDER = BASE_FOLDER / \"diff_files/\"\n",
    "\n",
    "# Make sure all the save folders exists\n",
    "RAW_FOLDER.mkdir(parents=True, exist_ok=True)    \n",
    "AGGREGATED_FOLDER.mkdir(parents=True, exist_ok=True)    \n",
    "BOTH_FOLDER.mkdir(parents=True, exist_ok=True)    \n",
    "DIFF_FOLDER.mkdir(parents=True, exist_ok=True)    \n",
    "\n",
    "# General file format for raw data\n",
    "RAW_FILE_TEMPLATE = \"mgra_id_DS{dsid}_{table}_ind_QA.csv\"\n",
    "CONSOLIDATED_FILE_TEMPLATE = \"{geo}_DS{dsid}_ind_QA.csv\"\n",
    "\n",
    "# The geographies to run on and the geographies properly formatted for a SQL query\n",
    "# NOTE: FORMATTED_GEOS starts with \"mgra.\", this is to distinguish between \"mgra.mgra_id\" and \n",
    "# \"{Estimates table name}.mgra_id\"\n",
    "GEOGRAPHIES = [\"mgra_id\", \"mgra\", \"cpa\", \"jurisdiction\", \"region\"]\n",
    "FORMATTED_GEOS = f\"mgra.{', '.join(GEOGRAPHIES)}\"\n",
    "\n",
    "# FORECAST_SERIES is the series that mgra_denormalize will be filtered with. It is important to note\n",
    "# that FORECAST_SERIES is 14, which rather confusingly corresponds to MGRA Series 13. When \n",
    "# we receive FORECAST_SERIES 15, it should correctly correspond to MGRA Series 15.\n",
    "# NOTE: This notebook makes the assumption that FORECAST_SERIES will applies to every id in \n",
    "# DATASOURCE_ID\n",
    "FORECAST_SERIES = 14\n",
    "DATASOURCE_IDS = [35, 38, 41, 42]\n",
    "\n",
    "# SQL queries to pull the unique categories from each dim table. These will be used \n",
    "category_queries = {\n",
    "    \"age\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT DISTINCT name\n",
    "        FROM [demographic_warehouse].[dim].[age_group]\n",
    "        ORDER BY name\n",
    "        \"\"\"),\n",
    "    \"ethnicity\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT DISTINCT long_name\n",
    "        FROM [demographic_warehouse].[dim].[ethnicity]\n",
    "        ORDER BY long_name\n",
    "        \"\"\"),\n",
    "    # TODO: should be income group (ex: income group 1 = i1)\n",
    "    \"household_income\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT DISTINCT name \n",
    "        FROM [demographic_warehouse].[dim].[income_group]\n",
    "        WHERE categorization = 10\n",
    "            AND constant_dollars_year = 2010\n",
    "        \"\"\"),\n",
    "    \"housing\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT DISTINCT long_name\n",
    "        FROM [demographic_warehouse].[dim].[structure_type]\n",
    "        ORDER BY long_name\n",
    "        \"\"\"),\n",
    "    \"jobs\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT DISTINCT full_name\n",
    "        FROM [demographic_warehouse].[dim].[employment_type]\n",
    "        ORDER BY full_name\n",
    "        \"\"\"),\n",
    "    # TODO: We should be using short_name\n",
    "    \"population\":textwrap.dedent(\"\"\"\\\n",
    "        SELECT DISTINCT long_name \n",
    "        FROM [demographic_warehouse].[dim].[housing_type]\n",
    "        ORDER BY long_name\n",
    "        \"\"\"),\n",
    "    \"sex\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT DISTINCT sex\n",
    "        FROM [demographic_warehouse].[dim].[sex]\n",
    "        ORDER BY sex\n",
    "        \"\"\"),\n",
    "}\n",
    "\n",
    "# SQL queries to get the requested table as a pivot table. Pivoting should not be done in Python,\n",
    "# since these tables are relatively large and you may run into memory issues. Since pivoting\n",
    "# is done in SQL, and because SQL pivoting requires the actual categorical values, the above \n",
    "# queries also need to exist.\n",
    "fact_queries = {\n",
    "    \"age\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT * FROM (\n",
    "            SELECT {geos}, yr_id, age_group.[name], [population] \n",
    "            FROM [demographic_warehouse].[fact].[age] as tbl\n",
    "            INNER JOIN [demographic_warehouse].[dim].[mgra_denormalize] AS mgra\n",
    "                ON mgra.mgra_id = tbl.mgra_id\n",
    "                AND mgra.series = {forecast_series}\n",
    "            INNER JOIN [demographic_warehouse].[dim].[age_group] as age_group\n",
    "                ON age_group.age_group_id = tbl.age_group_id\n",
    "            WHERE tbl.datasource_id = {dsid}) as p\n",
    "        PIVOT (\n",
    "            SUM([population])\n",
    "            FOR [name] IN (\n",
    "                {categories}\n",
    "            )\n",
    "        ) as pivot_table\n",
    "        ORDER BY mgra_id, yr_id\n",
    "        \"\"\"),\n",
    "    \"ethnicity\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT * FROM (\n",
    "            SELECT {geos}, yr_id, ethnicity.[long_name], [population] \n",
    "            FROM [demographic_warehouse].[fact].[ethnicity] as tbl\n",
    "            INNER JOIN [demographic_warehouse].[dim].[mgra_denormalize] AS mgra\n",
    "                ON mgra.mgra_id = tbl.mgra_id\n",
    "                AND mgra.series = {forecast_series}\n",
    "            INNER JOIN [demographic_warehouse].[dim].[ethnicity] as ethnicity\n",
    "                ON ethnicity.ethnicity_id = tbl.ethnicity_id\n",
    "            WHERE tbl.datasource_id = {dsid}) as p\n",
    "        PIVOT (\n",
    "            SUM([population])\n",
    "            FOR [long_name] IN (\n",
    "                {categories}\n",
    "            )\n",
    "        ) as pivot_table\n",
    "        ORDER BY mgra_id, yr_id\n",
    "        \"\"\"),\n",
    "    \"household_income\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT * FROM (\n",
    "            SELECT {geos}, yr_id, income_group.[name], [households] \n",
    "            FROM [demographic_warehouse].[fact].[household_income] as tbl\n",
    "            INNER JOIN [demographic_warehouse].[dim].[mgra_denormalize] AS mgra\n",
    "                ON mgra.mgra_id = tbl.mgra_id\n",
    "                AND mgra.series = {forecast_series}\n",
    "            INNER JOIN [demographic_warehouse].[dim].[income_group] as income_group\n",
    "                ON income_group.income_group_id = tbl.income_group_id\n",
    "            WHERE tbl.datasource_id = {dsid}) as p\n",
    "        PIVOT (\n",
    "            SUM([households])\n",
    "            FOR [name] IN (\n",
    "                {categories}\n",
    "            )\n",
    "        ) as pivot_table\n",
    "        ORDER BY mgra_id, yr_id\n",
    "        \"\"\"),\n",
    "    \"housing\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT * FROM (\n",
    "            SELECT {geos}, yr_id, structure_type.[long_name], [units] \n",
    "            FROM [demographic_warehouse].[fact].[housing] as tbl\n",
    "            INNER JOIN [demographic_warehouse].[dim].[mgra_denormalize] AS mgra\n",
    "                ON mgra.mgra_id = tbl.mgra_id\n",
    "                AND mgra.series = {forecast_series}\n",
    "            INNER JOIN [demographic_warehouse].[dim].[structure_type] as structure_type\n",
    "                ON structure_type.structure_type_id = tbl.structure_type_id\n",
    "            WHERE tbl.datasource_id = {dsid}) as p\n",
    "        PIVOT (\n",
    "            SUM([units])\n",
    "            FOR [long_name] IN (\n",
    "                {categories}\n",
    "            )\n",
    "        ) as pivot_table\n",
    "        ORDER BY mgra_id, yr_id\n",
    "        \"\"\"),\n",
    "    \"housing_units\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT {geos}, yr_id, \n",
    "            SUM([units]) as units, \n",
    "            SUM([unoccupiable]) as unoccupiable, \n",
    "            SUM([occupied]) as occupied, \n",
    "            SUM([vacancy]) as vacancy\n",
    "        FROM [demographic_warehouse].[fact].[housing] as tbl\n",
    "        INNER JOIN [demographic_warehouse].[dim].[mgra_denormalize] AS mgra\n",
    "            ON mgra.mgra_id = tbl.mgra_id\n",
    "            AND mgra.series = {forecast_series}\n",
    "        WHERE tbl.datasource_id = {dsid}\n",
    "        GROUP BY {geos}, yr_id\n",
    "        ORDER BY {geos}, yr_id\n",
    "        \"\"\"),\n",
    "    \"jobs\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT * FROM (\n",
    "            SELECT {geos}, yr_id, employment_type.[full_name], [jobs] \n",
    "            FROM [demographic_warehouse].[fact].[jobs] as tbl\n",
    "            INNER JOIN [demographic_warehouse].[dim].[mgra_denormalize] AS mgra\n",
    "                ON mgra.mgra_id = tbl.mgra_id\n",
    "                AND mgra.series = {forecast_series}\n",
    "            INNER JOIN [demographic_warehouse].[dim].[employment_type] as employment_type\n",
    "                ON employment_type.employment_type_id = tbl.employment_type_id\n",
    "            WHERE tbl.datasource_id = {dsid}) as p\n",
    "        PIVOT (\n",
    "            SUM([jobs])\n",
    "            FOR [full_name] IN (\n",
    "                {categories}\n",
    "            )\n",
    "        ) as pivot_table\n",
    "        ORDER BY mgra_id, yr_id\n",
    "        \"\"\"),\n",
    "    \"population\":textwrap.dedent(\"\"\"\\\n",
    "        SELECT * FROM (\n",
    "            SELECT {geos}, yr_id, housing_type.[long_name], [population] \n",
    "            FROM [demographic_warehouse].[fact].[population] as tbl\n",
    "            INNER JOIN [demographic_warehouse].[dim].[mgra_denormalize] AS mgra\n",
    "                ON mgra.mgra_id = tbl.mgra_id\n",
    "                AND mgra.series = {forecast_series}\n",
    "            INNER JOIN [demographic_warehouse].[dim].[housing_type] as housing_type\n",
    "                ON housing_type.housing_type_id = tbl.housing_type_id\n",
    "            WHERE tbl.datasource_id = {dsid}) as p\n",
    "        PIVOT (\n",
    "            SUM([population])\n",
    "            FOR [long_name] IN (\n",
    "                {categories}\n",
    "            )\n",
    "        ) as pivot_table\n",
    "        ORDER BY mgra_id, yr_id\n",
    "        \"\"\"),\n",
    "    \"sex\": textwrap.dedent(\"\"\"\\\n",
    "        SELECT * FROM (\n",
    "            SELECT {geos}, yr_id, sex.[sex], [population] \n",
    "            FROM [demographic_warehouse].[fact].[sex] as tbl\n",
    "            INNER JOIN [demographic_warehouse].[dim].[mgra_denormalize] AS mgra\n",
    "                ON mgra.mgra_id = tbl.mgra_id\n",
    "                AND mgra.series = {forecast_series}\n",
    "            INNER JOIN [demographic_warehouse].[dim].[sex] as sex\n",
    "                ON sex.sex_id = tbl.sex_id\n",
    "            WHERE tbl.datasource_id = {dsid}) as p\n",
    "        PIVOT (\n",
    "            SUM([population])\n",
    "            FOR [sex] IN (\n",
    "                {categories}\n",
    "            )\n",
    "        ) as pivot_table\n",
    "        ORDER BY mgra_id, yr_id\n",
    "        \"\"\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing LUZ Data Crosswalk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and save the MGRA -> LUZ crosswalk\n",
    "mgra_luz_crosswalk_query = textwrap.dedent(\"\"\"\\\n",
    "    SELECT [MGRA] ,[LUZ]\n",
    "    FROM [GeoDepot].[gis].[MGRA13]\n",
    "    \"\"\")\n",
    "mgra_luz_cw = pd.read_sql_query(mgra_luz_crosswalk_query, con=SQL2014B8)\n",
    "mgra_luz_cw.to_csv(RAW_FOLDER / \"mgra_luz_crosswalk.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting datasource_id=35, table=age\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=35, table=ethnicity\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=35, table=household_income\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=35, table=housing\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=35, table=housing_units\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=35, table=jobs\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=35, table=population\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=35, table=sex\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=38, table=age\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=38, table=ethnicity\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=38, table=household_income\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=38, table=housing\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=38, table=housing_units\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=38, table=jobs\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=38, table=population\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=38, table=sex\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=41, table=age\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=41, table=ethnicity\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=41, table=household_income\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=41, table=housing\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=41, table=housing_units\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=41, table=jobs\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=41, table=population\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=41, table=sex\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=42, table=age\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=42, table=ethnicity\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=42, table=household_income\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=42, table=housing\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=42, table=housing_units\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=42, table=jobs\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=42, table=population\n",
      "File already exists, skipping...\n",
      "\n",
      "Getting datasource_id=42, table=sex\n",
      "File already exists, skipping...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for datasource_id in DATASOURCE_IDS:\n",
    "    for table_name, query in fact_queries.items():\n",
    "\n",
    "        # Get and save the file\n",
    "        print(f\"Getting datasource_id={datasource_id}, table={table_name}\")\n",
    "\n",
    "        # Skip the file if it exists already\n",
    "        file_name = RAW_FOLDER / RAW_FILE_TEMPLATE.format(dsid=datasource_id, table=table_name)\n",
    "        if(file_name.is_file()):\n",
    "            print(\"File already exists, skipping...\")\n",
    "            \n",
    "        # If the file does not exist than download and save\n",
    "        else:\n",
    "            print(\"Getting table from DDAMWSQL16\")\n",
    "\n",
    "            # The housing units table is already in pivot table format, no need to get the \n",
    "            # categorical variables\n",
    "            if(table_name != \"housing_units\"):\n",
    "\n",
    "                # Get the list of categorical variables\n",
    "                cat_vars = [f\"[{x[0]}]\" for x in pd.read_sql_query(category_queries[table_name], con=DDAM).values]\n",
    "                cat_vars = \", \".join(cat_vars)\n",
    "\n",
    "                # Format the query\n",
    "                formatted_query = query.format(\n",
    "                    geos=FORMATTED_GEOS, \n",
    "                    forecast_series=FORECAST_SERIES, \n",
    "                    dsid=datasource_id,\n",
    "                    categories=cat_vars)\n",
    "                \n",
    "                # Actually get and save the file\n",
    "                table = pd.read_sql_query(formatted_query, con=DDAM)\n",
    "                \n",
    "                # The jobs table contains a bunch of extra years, remove them\n",
    "                if(table_name == \"jobs\"):\n",
    "                    years = pd.read_sql_query(textwrap.dedent(\"\"\"\\\n",
    "                        SELECT DISTINCT yr_id\n",
    "                        FROM [demographic_warehouse].[fact].[age]\n",
    "                        WHERE datasource_id = 41\n",
    "                        \"\"\"), con=DDAM)\n",
    "                    table = table[table[\"yr_id\"].isin(years[\"yr_id\"])]\n",
    "                table.to_csv(file_name, index=False)\n",
    "            \n",
    "            else:\n",
    "                # Custom behavior for the housing units table\n",
    "                formatted_query = query.format(\n",
    "                    geos=FORMATTED_GEOS, \n",
    "                    forecast_series=FORECAST_SERIES, \n",
    "                    dsid=datasource_id)\n",
    "                table = pd.read_sql_query(formatted_query, con=DDAM)\n",
    "                table.to_csv(file_name, index=False)\n",
    "\n",
    "            print(\"Completed\")\n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the files in the save folder\n",
    "files = [f for f in RAW_FOLDER.glob(\"**/*\") if f.is_file()]\n",
    "\n",
    "# Remove the MGRA -> LUZ crosswalk file\n",
    "files = [f for f in files if \"mgra_luz\" not in str(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS35_age_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS35_ethnicity_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS35_household_income_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS35_housing_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS35_housing_units_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS35_jobs_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS35_population_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS35_sex_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS38_age_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS38_ethnicity_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS38_household_income_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS38_housing_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS38_housing_units_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS38_jobs_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS38_population_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS38_sex_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS41_age_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS41_ethnicity_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS41_household_income_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS41_housing_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS41_housing_units_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS41_jobs_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS41_population_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS41_sex_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS42_age_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS42_ethnicity_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS42_household_income_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS42_housing_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS42_housing_units_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS42_jobs_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS42_population_ind_QA.csv\n",
      "No Errors\n",
      "\n",
      "Checking J:\\DataScience\\DataQuality\\QAQC\\forecast_automation\\mgra_series_13_outputs\\raw_data\\mgra_id_DS42_sex_ind_QA.csv\n",
      "No Errors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check that within each file...\n",
    "for file in files:\n",
    "    print(f\"Checking {file}\")\n",
    "    table = pd.read_csv(file)\n",
    "    errors = False\n",
    "\n",
    "    # Each mgra_id is associated with each year. In other words, the number of rows should be the \n",
    "    # number of distinct mgra_ids multiplied by the number of distinct yr_ids\n",
    "    num_mgra_id = len(table[\"mgra_id\"].unique())\n",
    "    num_yr_id = len(table[\"yr_id\"].unique())\n",
    "    if(num_mgra_id * num_yr_id != table.shape[0]):\n",
    "        errors = True\n",
    "        print(textwrap.dedent(f\"\"\"\\\n",
    "            {file} has {num_mgra_id} distinct mgra_ids and {num_yr_id} distinct num_yr_ids, so it \n",
    "            should have {num_mgra_id} x {num_yr_id} = {num_mgra_id * num_yr_id} rows of data. \n",
    "            However, {file} only has {table.shape[0]} rows of data.\n",
    "            \"\"\").replace(\"\\n\", \"\").replace(\"\\r\", \"\"))\n",
    "\n",
    "    # Each unique mgra_id appears in num_yr_id rows of data, and each unique yr_id appears in \n",
    "    # num_mgra_id rows of data\n",
    "    count = (table[\"mgra_id\"].value_counts() == num_yr_id).sum()\n",
    "    if(not count == num_mgra_id):\n",
    "        errors = True\n",
    "        print(textwrap.dedent(f\"\"\"\\\n",
    "            Each unique mgra_id should appear once for each distinct year, or {num_yr_id} times. \n",
    "            However, this only occurs for {count} mgra_ids instead of {num_mgra_id} mgra_ids.\n",
    "            \"\"\").replace(\"\\n\", \"\"))\n",
    "    count = (table[\"yr_id\"].value_counts() == num_mgra_id).sum()\n",
    "    if(not count == num_yr_id):\n",
    "        errors = True\n",
    "        print(textwrap.dedent(f\"\"\"\\\n",
    "            Each unique yr_id should appear once for each distinct mgra_id, or {num_mgra_id} times. \n",
    "            However, this only occurs for {count} yr_ids instead of {num_yr_id} yr_ids.\n",
    "            \"\"\").replace(\"\\n\", \"\"))\n",
    "\n",
    "    # Note the lack of errors if necessary\n",
    "    if(not errors):\n",
    "        print(\"No Errors\")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidating the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining files for datasource_id = 35\n",
      "Completed\n",
      "\n",
      "Combining files for datasource_id = 38\n",
      "Completed\n",
      "\n",
      "Combining files for datasource_id = 41\n",
      "Completed\n",
      "\n",
      "Combining files for datasource_id = 42\n",
      "Completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the files for each datasource\n",
    "for datasource_id in DATASOURCE_IDS:\n",
    "    \n",
    "    print(f\"Combining files for datasource_id = {datasource_id}\")\n",
    "\n",
    "    ds_files = [f for f in files if str(datasource_id) in str(f)]\n",
    "\n",
    "    # Get the tables\n",
    "    tables = []\n",
    "    for file in ds_files:\n",
    "        tables.append(pd.read_csv(file))\n",
    "\n",
    "    # Combine the tables together\n",
    "    consolidated = tables[0]\n",
    "    for i in range(1, len(tables)):\n",
    "        consolidated = pd.concat([consolidated, tables[i].drop(GEOGRAPHIES + [\"yr_id\"], axis=1)], axis=1)\n",
    "\n",
    "    # Save the file\n",
    "    file_name = CONSOLIDATED_FILE_TEMPLATE.format(geo=\"mgra_id\", dsid=datasource_id)\n",
    "    consolidated.to_csv(AGGREGATED_FOLDER / file_name, index=False)\n",
    "\n",
    "    print(\"Completed\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating from mgra_id to mgra, cpa, jurisdiction, region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating forecast 35 from \"mgra_id\" to mgra, cpa, jurisdiction, region\n",
      "Completed\n",
      "\n",
      "Aggregating forecast 38 from \"mgra_id\" to mgra, cpa, jurisdiction, region\n",
      "Completed\n",
      "\n",
      "Aggregating forecast 41 from \"mgra_id\" to mgra, cpa, jurisdiction, region\n",
      "Completed\n",
      "\n",
      "Aggregating forecast 42 from \"mgra_id\" to mgra, cpa, jurisdiction, region\n",
      "Completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the consolidated file for each datasource\n",
    "for datasource_id in DATASOURCE_IDS:\n",
    "    file_name = CONSOLIDATED_FILE_TEMPLATE.format(geo=\"mgra_id\", dsid=datasource_id)\n",
    "    consolidated = pd.read_csv(AGGREGATED_FOLDER / file_name)\n",
    "\n",
    "    # Aggregate up to every level except mgra_id\n",
    "    aggregate_geo_list = GEOGRAPHIES[:]\n",
    "    aggregate_geo_list.remove(\"mgra_id\")\n",
    "\n",
    "    print(f\"Aggregating forecast {datasource_id} from \\\"mgra_id\\\" to {', '.join(aggregate_geo_list)}\")\n",
    "\n",
    "    # Aggregate up to each geography level\n",
    "    for agg_geo in aggregate_geo_list:\n",
    "\n",
    "        # First select the columns which have actual data\n",
    "        # NOTE: This includes \"yr_id\"\n",
    "        data_cols = list(consolidated.columns[len(GEOGRAPHIES):])\n",
    "\n",
    "        # Then filter to only the geography we want and those data columns\n",
    "        aggregated = consolidated.copy(deep=True)[[agg_geo] + data_cols]\n",
    "\n",
    "        # Finally group by the geography and year\n",
    "        aggregated = aggregated.groupby([agg_geo, \"yr_id\"]).sum().reset_index(drop=False)\n",
    "\n",
    "        # Save the aggregated file with an appropriate name\n",
    "        agg_file_name = CONSOLIDATED_FILE_TEMPLATE.format(geo=agg_geo, dsid=datasource_id)\n",
    "        aggregated.to_csv(AGGREGATED_FOLDER / agg_file_name, index=False)\n",
    "\n",
    "        # Build in a mgra conditional LUZ output \n",
    "        if agg_geo == 'mgra':\n",
    "            mgra_luz_crosswalk = pd.read_csv(RAW_FOLDER / \"mgra_luz_crosswalk.csv\")\n",
    "            luz_table = aggregated.merge(mgra_luz_crosswalk, left_on='mgra', right_on='MGRA', how='left').drop(['mgra', 'MGRA'], axis=1)\n",
    "            luz_table = luz_table.groupby(['LUZ', \"yr_id\"]).sum().reset_index(drop=False)\n",
    "            \n",
    "            luz_file_name = CONSOLIDATED_FILE_TEMPLATE.format(geo=\"luz\", dsid=datasource_id)\n",
    "            luz_table.to_csv(AGGREGATED_FOLDER / luz_file_name, index=False)\n",
    "\n",
    "    print(\"Completed\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 8 minutes, 20 seconds\n"
     ]
    }
   ],
   "source": [
    "runtime = time.time() - start\n",
    "minutes, seconds = divmod(runtime, 60)\n",
    "print(f'Runtime: {int(minutes)} minutes, {(int(seconds))} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7804eeadc717885dfaa8f49b012a5fe9d8355528c06deeb48f3c48c16fd8060a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
