{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output section to make sure jurisdiction aggregation is done correctly on non-series 14 data (processed vs unprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Libraries \n",
    "import os\n",
    "import pyodbc\n",
    "import glob\n",
    "import copy\n",
    "import PySimpleGUI as sg\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MGRA Level Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate both DS dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dfs(comparison_first_ID_processed_data, comparison_second_ID_processed_data):\n",
    "    \"\"\"\n",
    "    Merges two mgra-level dataframes (generated by download_DS_data function) horizontally.\n",
    "    Returns a comparison table grouped by mgra and year.\n",
    "    \"\"\"\n",
    "    # Added geozone to merge keys to account for mgra's in multiple jurisdictions (or other geographical levels)\n",
    "    first_second_ID_comparison = comparison_first_ID_processed_data.merge(\n",
    "        comparison_second_ID_processed_data,\n",
    "        how='outer',\n",
    "        left_on=[f'mgra_{first_ID}',\n",
    "                 f'year_{first_ID}',\n",
    "                 f'geozone_{first_ID}'],\n",
    "        right_on=[f'mgra_{second_ID}',\n",
    "                 f'year_{second_ID}',\n",
    "                 f'geozone_{second_ID}'])\n",
    "    \n",
    "    # Clean green combined\n",
    "    first_second_ID_comparison = first_second_ID_comparison.drop([f'mgra_{second_ID}', f'year_{second_ID}', f'geozone_{second_ID}'], axis=1)\n",
    "    first_second_ID_comparison = first_second_ID_comparison.rename(columns={f'mgra_{first_ID}': 'mgra', f'year_{first_ID}': 'year', f'geozone_{first_ID}': 'geozone'})\n",
    "    \n",
    "    # Because we're summing, if using series 14 data, mgra's in multiple jurisdictions will be counted multiple times\n",
    "    first_second_ID_comparison = first_second_ID_comparison.groupby(['mgra', 'year']).sum()\n",
    "        \n",
    "    return first_second_ID_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dfs_temp(comparison_first_ID_processed_data, comparison_second_ID_processed_data):\n",
    "    \"\"\"\n",
    "    Merges two mgra-level dataframes (generated by download_DS_data function) horizontally.\n",
    "    Returns a comparison table grouped by mgra and year.\n",
    "    \"\"\"\n",
    "    # Added geozone to merge keys to account for mgra's in multiple jurisdictions (or other geographical levels)\n",
    "    first_second_ID_comparison = comparison_first_ID_processed_data.merge(\n",
    "        comparison_second_ID_processed_data,\n",
    "        how='outer',\n",
    "        on=[f'mgra', f'year'],\n",
    "        suffixes=[f'_{first_ID}', f'_{second_ID}'])\n",
    "    \n",
    "    #print(first_second_ID_comparison)\n",
    "    \n",
    "    # Clean green combined\n",
    "    #first_second_ID_comparison = first_second_ID_comparison.drop([f'mgra_DS41', f'year_DS41'], axis=1)\n",
    "    #first_second_ID_comparison = first_second_ID_comparison.rename(columns={f'mgra_DS35': 'mgra', f'year_DS35': 'year'})\n",
    "    \n",
    "    # Because we're summing, if using series 14 data, mgra's in multiple jurisdictions will be counted multiple times\n",
    "    first_second_ID_comparison = first_second_ID_comparison.groupby(['mgra', 'year']).sum()\n",
    "        \n",
    "    return first_second_ID_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPA level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpa_aggregation(first_ID_df, second_ID_df, cpa_level):\n",
    "    \"\"\"\n",
    "    Joins Community Planning Area (CPA) information onto MGRA-level dataframes (generated by download_DS_data function).\n",
    "    Drops MGRA values that aren't in a CPA.\n",
    "    Returns a comparison table grouped by CPA and year.\n",
    "    \"\"\"\n",
    "    # Adding SQl Data (CPA) to first_id_df\n",
    "    comparison_first_ID_processed_data_cpa = first_ID_df.merge(cpa_level, how='left', on='mgra')\n",
    "    comparison_first_ID_processed_data_cpa = comparison_first_ID_processed_data_cpa[comparison_first_ID_processed_data_cpa['geozone'] != '*Not in a CPA*']\n",
    "\n",
    "    # Adding SQl Data (CPA) to second_id_df\n",
    "    comparison_second_ID_processed_data_cpa = second_ID_df.merge(cpa_level, how='left', on='mgra')\n",
    "    comparison_second_ID_processed_data_cpa = comparison_second_ID_processed_data_cpa[comparison_second_ID_processed_data_cpa['geozone'] != '*Not in a CPA*']\n",
    "\n",
    "    # Merge first_id_df and second_id_df together on mgra, year, and geozone\n",
    "    comparison_processed_data_cpa = comparison_first_ID_processed_data_cpa.merge(comparison_second_ID_processed_data_cpa, how='outer', on=['mgra', 'year', 'geozone'], suffixes=[f'_{first_ID}', f'_{second_ID}'])\n",
    "\n",
    "    # Drop the MGRA column because it isn't really a quantitative value\n",
    "    comparison_processed_data_cpa = comparison_processed_data_cpa.drop('mgra', axis=1)\n",
    "\n",
    "    # Aggregate the sum of features by geozone and year\n",
    "    comparison_processed_data_cpa = comparison_processed_data_cpa.groupby(['geozone', 'year']).sum()\n",
    "\n",
    "    # Rename index (geozone -> cpa)\n",
    "    comparison_processed_data_cpa.index.names = ['cpa', 'year']\n",
    "    \n",
    "    return comparison_processed_data_cpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jurisdiction level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jur_aggregation(first_ID_df, second_ID_df, jur_level):\n",
    "    \"\"\"\n",
    "    Joins Jurisdiction information onto MGRA-level dataframes (generated by download_DS_data function).\n",
    "    Returns a comparison table grouped by Jurisdiction and year.\n",
    "    \"\"\"\n",
    "    # Adding SQl Data (Jurisdiction) to first_id_df\n",
    "    comparison_first_ID_processed_data_jur = first_ID_df.merge(jur_level, how='left', on='mgra')\n",
    "    \n",
    "    # Adding SQl Data (Jurisdiction) to second_id_df\n",
    "    comparison_second_ID_processed_data_jur = second_ID_df.merge(jur_level, how='left', on='mgra')\n",
    "    \n",
    "    # Merge first_id_df and second_id_df together on mgra, year, and geozone\n",
    "    comparison_processed_data_jur = comparison_first_ID_processed_data_jur.merge(comparison_second_ID_processed_data_jur, how='outer', on=['mgra', 'year', 'geozone'], suffixes=[f'_{first_ID}', f'_{second_ID}'])\n",
    "    \n",
    "    # Drop the MGRA column because it isn't really a quantitative value\n",
    "    comparison_processed_data_jur = comparison_processed_data_jur.drop('mgra', axis=1)\n",
    "    \n",
    "    # Aggregate the sum of features by geozone and year\n",
    "    comparison_processed_data_jur = comparison_processed_data_jur.groupby(['geozone', 'year']).sum()\n",
    "    \n",
    "    # Rename index (geozone -> jurisdiction)\n",
    "    comparison_processed_data_jur.index.names = ['jurisdiction', 'year']\n",
    "        \n",
    "    return comparison_processed_data_jur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Diff File for all Geo Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_shared_features(features_first_ID, features_second_ID):\n",
    "    \"\"\"\n",
    "    (Comparison only)\n",
    "    Identifies non-shared features between two different DS_ID's.\n",
    "    \"\"\"\n",
    "    # Display non-shared features\n",
    "    return list(set(features_first_ID) ^ set(features_second_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_shared_years(first_ID_df, second_ID_df):\n",
    "    \"\"\"\n",
    "    (Comparison only)\n",
    "    Identifies non-shared years between two different DS_ID's.\n",
    "    \"\"\"\n",
    "    # Display non-shared years\n",
    "    return set(list(first_ID_df['year'].unique())) ^ set(list(second_ID_df['year'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diff(features_first_ID, features_second_ID, first_second_ID_comparison):\n",
    "    \"\"\"\n",
    "    (Comparison only)\n",
    "    Returns a comparison table where the second_ID values are subtracted from the first_ID values.\n",
    "    \"\"\"\n",
    "    # Finding features common to both DSID data frames\n",
    "    first_ID_unique = set(features_first_ID)\n",
    "    intersection = first_ID_unique.intersection(features_second_ID)\n",
    "    shared_features = list(intersection)\n",
    "    \n",
    "    # Calculate diff values between the two DS_ID's\n",
    "    diff_df = pd.DataFrame()\n",
    "\n",
    "    # NOTE: Subtracts second DS ID from first DS ID. If negative, then second DS ID was greater than first DS ID.\n",
    "    for column in [col for col in features_first_ID if col in features_second_ID]:\n",
    "        diff_df[f'{column}_diff'] = first_second_ID_comparison[f'{column}_{first_ID}'] - first_second_ID_comparison[f'{column}_{second_ID}']\n",
    "        \n",
    "    return diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diff_temp(features_first_ID, features_second_ID, first_second_ID_comparison):\n",
    "    \"\"\"\n",
    "    (Comparison only)\n",
    "    Returns a comparison table where the second_ID values are subtracted from the first_ID values.\n",
    "    \"\"\"\n",
    "    # Finding features common to both DSID data frames\n",
    "    #first_ID_unique = set(features_first_ID)\n",
    "    #intersection = first_ID_unique.intersection(features_second_ID)\n",
    "    \n",
    "    shared_feats = [col for col in features_first_ID if col in features_second_ID]\n",
    "    \n",
    "    #shared_features = list(intersection)\n",
    "    \n",
    "    # Calculate diff values between the two DS_ID's\n",
    "    diff_df = pd.DataFrame()\n",
    "\n",
    "    # NOTE: Subtracts second DS ID from first DS ID. If negative, then second DS ID was greater than first DS ID.\n",
    "    for column in shared_feats:\n",
    "        diff_df[f'{column}_diff'] = first_second_ID_comparison[f'{column}_{first_ID}'] - first_second_ID_comparison[f'{column}_{second_ID}']\n",
    "        \n",
    "    return diff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_aggregation(first_ID_df, second_ID_df):\n",
    "    \"\"\"\n",
    "    Sums the entire MGRA-level dataframes (generated by download_DS_data function) by column to get region values.\n",
    "    Returns a comparison table grouped by year.\n",
    "    \"\"\"\n",
    "    # Merge first_id_df and second_id_df together on mgra and year\n",
    "    comparison_processed_data_reg = first_ID_df.merge(second_ID_df, how='outer', on=['mgra', 'year'], suffixes=[f'_{first_ID}', f'_{second_ID}'])\n",
    "    \n",
    "    # Aggregate the sum of features by year\n",
    "    comparison_processed_data_reg = comparison_processed_data_reg.groupby('year').sum()\n",
    "    \n",
    "    # Drop the MGRA column because it isn't really a quantitative value\n",
    "    comparison_processed_data_reg = comparison_processed_data_reg.drop('mgra', axis=1)\n",
    "        \n",
    "    return comparison_processed_data_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_DS_data(ds_ID, jur_level):\n",
    "    \"\"\"\n",
    "    Downloads DS_ID csv data from SANDAG's T drive, formatted for non-MGRA series 14 data.\n",
    "    Returns processed data (merged with jurisdiction data and DS labeled), unprocessed data, and the features in the\n",
    "    dataset.\n",
    "    \"\"\"\n",
    "    datafiles = list(config[ds_ID].items())[:-1]\n",
    "    \n",
    "    comparison_no_geozone_df = pd.DataFrame()\n",
    "    for year, file_name in datafiles:\n",
    "        working_df = pd.read_csv(file_name)\n",
    "        working_df['year'] = int(year[-4:])\n",
    "        comparison_no_geozone_df = pd.concat([comparison_no_geozone_df, working_df], ignore_index=True)\n",
    "\n",
    "    # rename columns from sql data\n",
    "    land_use_key = {}\n",
    "    for col in comparison_no_geozone_df.columns:\n",
    "        new_col = col.split('_')\n",
    "        for key, value in land_use_key_incomplete.items():\n",
    "            if key in new_col:\n",
    "                my_list = np.array(new_col)\n",
    "                my_list = np.where(my_list == key, value, my_list)\n",
    "                land_use_key[col] = '_'.join(my_list).replace(' ', '_')\n",
    "    \n",
    "    comparison_no_geozone_df = comparison_no_geozone_df.rename(columns=housing_key)\n",
    "    comparison_no_geozone_df = comparison_no_geozone_df.rename(columns=income_key)\n",
    "    comparison_no_geozone_df = comparison_no_geozone_df.rename(columns=land_use_key)\n",
    "    \n",
    "    employment_key = {}\n",
    "    for col in comparison_no_geozone_df.columns:\n",
    "        new_col = col.split('_')\n",
    "        for key, value in employment_key_incomplete.items():\n",
    "            if key in new_col:\n",
    "                my_list = np.array(new_col)\n",
    "                my_list = np.where(my_list == key, value, my_list)\n",
    "                employment_key[col] = '_'.join(my_list).replace(' ', '_')\n",
    "            \n",
    "    comparison_no_geozone_df = comparison_no_geozone_df.rename(columns=employment_key)\n",
    "    \n",
    "    # conn needs to be in function because conn can expire and need to be re-run\n",
    "    conn = pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};'\n",
    "                      'Server=DDAMWSQL16.sandag.org;'\n",
    "                      'Database=demographic_warehouse;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "    \n",
    "    comparison_no_geozone_df = mgra_vacancy_additions(comparison_no_geozone_df, ds_ID[-2:], forecast_series_ID=config[ds_ID]['forecast_ID'], conn=conn)\n",
    "    comparison_no_geozone_df = mgra_school_pop_additions(comparison_no_geozone_df, ds_ID[-2:], forecast_series_ID=config[ds_ID]['forecast_ID'], conn=conn)\n",
    "    comparison_no_geozone_df = mgra_age_pop_additions(comparison_no_geozone_df, ds_ID[-2:], forecast_series_ID=config[ds_ID]['forecast_ID'], conn=conn)\n",
    "    comparison_no_geozone_df = mgra_ethn_pop_additions(comparison_no_geozone_df, ds_ID[-2:], forecast_series_ID=config[ds_ID]['forecast_ID'], conn=conn)\n",
    "    comparison_no_geozone_df = mgra_sex_pop_additions(comparison_no_geozone_df, ds_ID[-2:], forecast_series_ID=config[ds_ID]['forecast_ID'], conn=conn)\n",
    "                \n",
    "    comparison_no_geozone = copy.deepcopy(comparison_no_geozone_df)\n",
    "    \n",
    "    # Save the features_first_ID for future use (Used when creating the diff file)\n",
    "    features = comparison_no_geozone_df.drop(['mgra', 'year'], axis=1).columns\n",
    "    \n",
    "    # Adding SQl Data to first_id_df\n",
    "    comparison_processed_data = comparison_no_geozone.merge(jur_level, how='left', on='mgra')\n",
    "    \n",
    "    # making it original\n",
    "    comparison_processed_data.columns = [x + f'_{ds_ID}' for x in comparison_processed_data.columns]\n",
    "    \n",
    "    if comparison_processed_data.shape[0] != comparison_no_geozone_df.shape[0]:\n",
    "        print(f\"{ds_ID}'s jurisdiction did not merge correctly.\")\n",
    "        \n",
    "    return comparison_processed_data, comparison_no_geozone_df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPA Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpa_aggregation_ind(first_ID_df, cpa_level):\n",
    "    \"\"\"\n",
    "    Joins Community Planning Area (CPA) information onto an MGRA-level dataframe (generated by download_DS_data function).\n",
    "    Drops MGRA values that aren't in a CPA.\n",
    "    Returns a table containing aggregated CPA values grouped by CPA and year.\n",
    "    \"\"\"\n",
    "    # Adding SQl Data (CPA) to first_id_df\n",
    "    comparison_first_ID_processed_data_cpa = first_ID_df.merge(cpa_level, how='left', on='mgra')\n",
    "    comparison_first_ID_processed_data_cpa = comparison_first_ID_processed_data_cpa[comparison_first_ID_processed_data_cpa['geozone'] != '*Not in a CPA*']\n",
    "\n",
    "    # Drop the MGRA column because it isn't really a quantitative value\n",
    "    comparison_processed_data_cpa = comparison_first_ID_processed_data_cpa.drop('mgra', axis=1)\n",
    "\n",
    "    # Aggregate the sum of features by geozone and year\n",
    "    comparison_processed_data_cpa = comparison_processed_data_cpa.groupby(['geozone', 'year']).sum()\n",
    "\n",
    "    # Rename index (geozone -> cpa)\n",
    "    comparison_processed_data_cpa.index.names = ['cpa', 'year']\n",
    "    \n",
    "    return comparison_processed_data_cpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jurisdiction level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jur_aggregation_ind(first_ID_df, jur_level):\n",
    "    \"\"\"\n",
    "    Joins Jurisdiction information onto an MGRA-level dataframe (generated by download_DS_data function).\n",
    "    Returns a table containing aggregated jurisdiction values grouped by jurisdiction and year.\n",
    "    \"\"\"\n",
    "    # Adding SQl Data (Jurisdiction) to first_id_df\n",
    "    comparison_first_ID_processed_data_jur = first_ID_df.merge(jur_level, how='left', on='mgra')\n",
    "    \n",
    "    # Drop the MGRA column because it isn't really a quantitative value\n",
    "    comparison_processed_data_jur = comparison_first_ID_processed_data_jur.drop('mgra', axis=1)\n",
    "    \n",
    "    # Aggregate the sum of features by geozone and year\n",
    "    comparison_processed_data_jur = comparison_processed_data_jur.groupby(['geozone', 'year']).sum()\n",
    "    \n",
    "    # Rename index (geozone -> jurisdiction)\n",
    "    comparison_processed_data_jur.index.names = ['jurisdiction', 'year']\n",
    "        \n",
    "    return comparison_processed_data_jur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_aggregation_ind(first_ID_df):\n",
    "    \"\"\"\n",
    "    Sums the entire MGRA-level dataframe (generated by download_DS_data function) by column to get region values.\n",
    "    Returns a table containing aggregated mgra values grouped by year.\n",
    "    \"\"\"\n",
    "    # Aggregate the sum of features by year\n",
    "    comparison_processed_data_reg = first_ID_df.groupby('year').sum()\n",
    "    \n",
    "    # Drop the MGRA column because it isn't really a quantitative value\n",
    "    comparison_processed_data_reg = comparison_processed_data_reg.drop('mgra', axis=1)\n",
    "        \n",
    "    return comparison_processed_data_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Info From YML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Localise with . files \n",
    "config_filename = './config_files/ds_config.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_filename, \"r\") as yml_file:\n",
    "    config = yaml.safe_load(yml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading SQL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};'\n",
    "                      'Server=DDAMWSQL16.sandag.org;'\n",
    "                      'Database=demographic_warehouse;'\n",
    "                      'Trusted_Connection=yes;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_all = \"SELECT mgra, geotype, geozone FROM demographic_warehouse.dim.mgra WHERE series = 14 AND (geotype='cpa' OR geotype='jurisdiction')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = pd.read_sql_query(query_all,conn)\n",
    "sql_df_all = pd.DataFrame(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQl Data at different levels\n",
    "jur_level = sql_df_all[sql_df_all['geotype']=='jurisdiction'].drop('geotype', axis=1).drop_duplicates()\n",
    "cpa_level = sql_df_all[sql_df_all['geotype']=='cpa'].drop('geotype', axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hh, gq_mil, gq_college, and gq_other sql query\n",
    "housing_query = \"SELECT short_name, long_name FROM demographic_warehouse.dim.housing_type\"\n",
    "income_query = \"SELECT TOP(10) [income_group], [name], [constant_dollars_year]\\\n",
    " FROM [demographic_warehouse].[dim].[income_group]\\\n",
    " WHERE [constant_dollars_year] = 2010\"\n",
    "land_use_query = \"SELECT [land_use_type_id]\\\n",
    "      ,[short_name]\\\n",
    "      ,[long_name]\\\n",
    "  FROM [demographic_warehouse].[dim].[land_use_type]\"\n",
    "employment_query = \"SELECT TOP (1000) [employment_type_id]\\\n",
    "      ,[short_name]\\\n",
    "      ,[full_name]\\\n",
    "      ,[civilian]\\\n",
    "  FROM [demographic_warehouse].[dim].[employment_type]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_info = pd.read_sql_query(housing_query,conn)\n",
    "sql_housing_info = pd.DataFrame(housing_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important to avoid Latex formatting (with dollar signs)\n",
    "pd.options.display.html.use_mathjax = False\n",
    "income_info = pd.read_sql_query(income_query, conn)[['income_group', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_use_info = pd.read_sql_query(land_use_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_info = pd.read_sql_query(employment_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_key = sql_housing_info.set_index('short_name').to_dict()['long_name']\n",
    "income_key_incomplete = income_info.set_index('income_group').to_dict()['name']\n",
    "land_use_key_incomplete = land_use_info.set_index('short_name').to_dict()['long_name']\n",
    "employment_key_incomplete = employment_info.set_index('short_name').to_dict()['full_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in housing_key.items():\n",
    "    housing_key[key] = f'{value} ({key})'\n",
    "    \n",
    "income_key = {}\n",
    "for key, value in income_key_incomplete.items():\n",
    "    income_key[f'i{key}'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mgra_vacancy_additions(mgra_df, ds_id, forecast_series_ID, conn): \n",
    "    # This series 14 corresponds to the forecast version \n",
    "    # that we are using (Which used series 13 of the MGRAS). If I am looking at MGRA from a GIS layer, that is based \n",
    "    # on series 13 of MGRA. Series 14 MGRA was never used. Series 14 of the forecasts will have MGRA_ID because \n",
    "    # Series 13 mgra has an issue.\n",
    "    dim_mgra = f\"SELECT [mgra_id], [mgra], [cpa], [cpa_id], [jurisdiction] FROM [demographic_warehouse].\\\n",
    "[dim].[mgra_denormalize] WHERE series={forecast_series_ID}\" \n",
    "    d_mgra= pd.read_sql_query(dim_mgra, conn)\n",
    "\n",
    "    housing= f\"SELECT [housing_id],[datasource_id],[yr_id],[mgra_id],[structure_type_id],[units],[occupied],\\\n",
    "[vacancy],[unoccupiable] FROM [demographic_warehouse].[fact].[housing] WHERE datasource_id={ds_id}\"\n",
    "    d_housing= pd.read_sql_query(housing, conn)\n",
    "\n",
    "    merged = d_housing.merge(d_mgra, how='left', on='mgra_id')\n",
    "    \n",
    "    # MGRA\n",
    "    mgra_vacancy = merged[['mgra', 'yr_id', 'units', 'vacancy', 'unoccupiable']]\n",
    "    mgra_vacancy = mgra_vacancy.groupby(['mgra', 'yr_id']).sum()\n",
    "    mgra_vacancy['vacancy_rate'] = ((mgra_vacancy['vacancy'] - mgra_vacancy['unoccupiable']) / mgra_vacancy['units']) * 100\n",
    "    mgra_vacancy.index = mgra_vacancy.index.set_names('year', level=1)\n",
    "    \n",
    "    # Merge this dataframe with the mgra_df\n",
    "    return mgra_df.merge(mgra_vacancy, on=['mgra','year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mgra_school_pop_additions(mgra_df, ds_id, forecast_series_ID, conn): \n",
    "    # From the dim table bring in information on elementary,secondary, and school district info. \n",
    "    query = \"SELECT * FROM [demographic_warehouse].[dim].[mgra_denormalize]\"\n",
    "    school_data = pd.read_sql_query(query, conn)\n",
    "\n",
    "    s_14_school_data = school_data[school_data['series']==forecast_series_ID] # This is forecast 14\n",
    "\n",
    "    mgra_school_data = s_14_school_data[['mgra_id', 'mgra', 'secondary', 'elementary','unified']]\n",
    "\n",
    "    # This is creating a new column that tells us which information is present in regards to school district, secondary, elementary info.\n",
    "    conditions = [\n",
    "        (mgra_school_data['secondary'].isna()) & (mgra_school_data['elementary'].isna()) & (~mgra_school_data['unified'].isna()),\n",
    "        (~mgra_school_data['secondary'].isna()) & (~mgra_school_data['elementary'].isna()) & (mgra_school_data['unified'].isna())\n",
    "    ]\n",
    "\n",
    "    values = ['Only Unified', 'S&E No Unified']\n",
    "\n",
    "    mgra_school_data.loc[:, 'School Data Present'] = np.select(conditions, values)\n",
    "\n",
    "    mgra_school_data = mgra_school_data.reset_index()\n",
    "\n",
    "    # Here we are bringing in from the fact table, age information  \n",
    "    conn = pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};'\n",
    "                    'Server=ddamwsql16.sandag.org;'\n",
    "                    'Database=demographic_warehouse;'\n",
    "                    'Trusted_Connection=yes;')\n",
    "\n",
    "    age_group_breakdown_query = f\"SELECT [yr_id],[mgra_id],[age_group_id],[population] FROM [demographic_warehouse]\\\n",
    ".[fact].[age] WHERE [age_group_id] IN (1,2,3,4,5) AND [yr_id] >= 2016 AND [datasource_id] = {ds_id};\"\n",
    "\n",
    "    age_group_breakdown = pd.read_sql_query(age_group_breakdown_query, conn)\n",
    "\n",
    "    # Using the names of the category IDs (Found: [demographic_warehouse].[dim].[age_group]) this will rename the categories to their actual names\n",
    "    conditions_2 = [\n",
    "        (age_group_breakdown['age_group_id'] == 1),\n",
    "        (age_group_breakdown['age_group_id'] == 2),\n",
    "        (age_group_breakdown['age_group_id'] == 3),\n",
    "        (age_group_breakdown['age_group_id'] == 4),\n",
    "        (age_group_breakdown['age_group_id'] == 5)\n",
    "        ]\n",
    "\n",
    "    values_2 = ['Under 5', '5 to 9', '10 to 14', '15 to 17', '18 to 19']\n",
    "\n",
    "    age_group_breakdown.loc[:, 'Age Group'] = np.select(conditions_2, values_2)\n",
    "\n",
    "    age_group_breakdown = age_group_breakdown.drop('age_group_id', axis=1)\n",
    "\n",
    "    # Joining age with school data\n",
    "    age_school_combined = age_group_breakdown.merge(mgra_school_data, on='mgra_id', how='left')\n",
    "\n",
    "    final_school_age_combo = age_school_combined.set_index(['mgra','yr_id']) # This is for DSID 35\n",
    "\n",
    "    # Splitting into different population: High school age and Elementary school age\n",
    "    elem, high = ['Under 5', '5 to 9', '10 to 14'], ['15 to 17', '18 to 19']\n",
    "    school_pop = pd.DataFrame()\n",
    "\n",
    "    school_pop.loc[:, 'elem_population'] = final_school_age_combo[final_school_age_combo['Age Group'].isin(elem)].reset_index().groupby(['mgra', 'yr_id']).sum()['population']\n",
    "    school_pop.loc[:, 'high_population'] = final_school_age_combo[final_school_age_combo['Age Group'].isin(high)].reset_index().groupby(['mgra', 'yr_id']).sum()['population']\n",
    "\n",
    "    school_pop.index = school_pop.index.set_names('year', level=1)\n",
    "\n",
    "    # Merge this dataframe with the mgra_df\n",
    "    return mgra_df.merge(school_pop, on=['mgra','year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mgra_age_pop_additions(mgra_df, ds_id, forecast_series_ID, conn):\n",
    "    # This series 14 corresponds to the forecast version \n",
    "    # that we are using (Which used series 13 of the MGRAS). If I am looking at MGRA from a GIS layer, that is based \n",
    "    # on series 13 of MGRA. Series 14 MGRA was never used. Series 14 of the forecasts will have MGRA_ID because \n",
    "    # Series 13 mgra has an issue.\n",
    "    dim_mgra = f\"SELECT [mgra_id], [mgra], [cpa], [cpa_id], [jurisdiction] FROM [demographic_warehouse].\\\n",
    "[dim].[mgra_denormalize]  WHERE series={forecast_series_ID}\" \n",
    "    d_mgra= pd.read_sql_query(dim_mgra, conn)\n",
    "\n",
    "    # Age population query \n",
    "    age_query = f\"SELECT [yr_id],[mgra_id],dim.name,[population] FROM [demographic_warehouse].[fact].[age]\\\n",
    "AS fact LEFT JOIN dim.age_group AS dim ON dim.age_group_id = fact.age_group_id WHERE \\\n",
    "datasource_id = {ds_id};\"\n",
    "\n",
    "    age = pd.read_sql_query(age_query, conn)\n",
    "\n",
    "    # merge to add mgra_id / mgra values. Pivot to group mgra and yr_id as index, name as columns, and population as values\n",
    "    age_merged = age.merge(d_mgra, on='mgra_id')\n",
    "    age_pop = age_merged.pivot_table(columns='name', index=['mgra', 'yr_id'], values='population', aggfunc='sum')\n",
    "    age_pop.index = age_pop.index.set_names('year', level=1) # rename index to join with mgra dataframe\n",
    "\n",
    "    return mgra_df.merge(age_pop, on=['mgra','year'], how='left')\n",
    "\n",
    "def mgra_ethn_pop_additions(mgra_df, ds_id, forecast_series_ID, conn):\n",
    "    # This series 14 corresponds to the forecast version \n",
    "    # that we are using (Which used series 13 of the MGRAS). If I am looking at MGRA from a GIS layer, that is based \n",
    "    # on series 13 of MGRA. Series 14 MGRA was never used. Series 14 of the forecasts will have MGRA_ID because \n",
    "    # Series 13 mgra has an issue.\n",
    "    dim_mgra = f\"SELECT [mgra_id], [mgra], [cpa], [cpa_id], [jurisdiction] FROM [demographic_warehouse].\\\n",
    "[dim].[mgra_denormalize]  WHERE series={forecast_series_ID}\" \n",
    "    d_mgra= pd.read_sql_query(dim_mgra, conn)\n",
    "\n",
    "    # Ethnicity population query \n",
    "    ethn_query = f\"SELECT [yr_id],[mgra_id],dim.short_name,[population] FROM [demographic_warehouse].[fact].[ethnicity]\\\n",
    "AS fact LEFT JOIN [demographic_warehouse].[dim].[ethnicity] AS dim ON fact.ethnicity_id = dim.ethnicity_id WHERE \\\n",
    "fact.datasource_id = {ds_id};\"\n",
    "\n",
    "    ethn = pd.read_sql_query(ethn_query, conn)\n",
    "\n",
    "    # merge to add mgra_id / mgra values. Pivot to group mgra and yr_id as index, short_name as columns, and population as values\n",
    "    ethn_merged = ethn.merge(d_mgra, on='mgra_id')\n",
    "    ethn_pop = ethn_merged.pivot_table(columns='short_name', index=['mgra', 'yr_id'], values='population', aggfunc='sum')\n",
    "    ethn_pop.index = ethn_pop.index.set_names('year', level=1) # rename index to join with mgra dataframe\n",
    "\n",
    "    return mgra_df.merge(ethn_pop, on=['mgra','year'], how='left')\n",
    "\n",
    "def mgra_sex_pop_additions(mgra_df, ds_id, forecast_series_ID, conn):\n",
    "    # This series 14 corresponds to the forecast version \n",
    "    # that we are using (Which used series 13 of the MGRAS). If I am looking at MGRA from a GIS layer, that is based \n",
    "    # on series 13 of MGRA. Series 14 MGRA was never used. Series 14 of the forecasts will have MGRA_ID because \n",
    "    # Series 13 mgra has an issue.\n",
    "    dim_mgra = f\"SELECT [mgra_id], [mgra], [cpa], [cpa_id], [jurisdiction] FROM [demographic_warehouse].\\\n",
    "[dim].[mgra_denormalize]  WHERE series={forecast_series_ID}\" \n",
    "    d_mgra= pd.read_sql_query(dim_mgra, conn)\n",
    "\n",
    "    # Sex population query \n",
    "    sex_query = f\"SELECT [yr_id],[mgra_id],dim.sex,[population] FROM [demographic_warehouse].[fact].[sex]\\\n",
    "AS fact LEFT JOIN dim.sex AS dim ON fact.sex_id = dim.sex_id WHERE fact.datasource_id = {ds_id};\"\n",
    "\n",
    "    sex = pd.read_sql_query(sex_query, conn)\n",
    "\n",
    "    # merge to add mgra_id / mgra values. Pivot to group mgra and yr_id as index, short_name as columns, and \n",
    "    # aggregate by summing the population values\n",
    "    sex_merged = sex.merge(d_mgra, on='mgra_id')\n",
    "    sex_pop = sex_merged.pivot_table(columns='sex', index=['mgra', 'yr_id'], values='population', aggfunc='sum')\n",
    "    sex_pop.index = sex_pop.index.set_names('year', level=1) # rename index to join with mgra dataframe\n",
    "\n",
    "    return mgra_df.merge(sex_pop, on=['mgra','year'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare desired output options\n",
    "comparison_selection_list = ['mgra_both', 'cpa_both', 'jur_both', 'region_both', 'mgra_diff', 'cpa_diff', 'jur_diff', 'region_diff']\n",
    "individual_selection_list = ['mgra_ind', 'cpa_ind', 'jur_ind', 'region_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_window():\n",
    "    \"\"\"\n",
    "    Creates SimplePyGUI window that enables user to select output path and output option (comparison or individual).\n",
    "    Returns click event as well as selected values (click event will indicate output option and values will indicate \n",
    "    output path).\n",
    "    \"\"\"\n",
    "    layout_first = [ \n",
    "        [sg.Text('Please Designate An Output Path (or leave blank to use local outputs folder)')],\n",
    "        [sg.Text('Output Path', size =(15, 1)), sg.FolderBrowse(key='output-path')],\n",
    "        [sg.Text('Select An Output Option')],\n",
    "        [sg.Button(button_text='Comparison', key='comparison-select'),\n",
    "         sg.Button(button_text='Individual', key='individual-select'),\n",
    "         sg.Cancel()]\n",
    "    ]\n",
    "    \n",
    "    window = sg.Window('Base window', layout_first, element_justification='c')\n",
    "    event, values = window.read()\n",
    "    window.close()\n",
    "\n",
    "    return event, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_inputs(event, values, output_path, output_notes):\n",
    "    \"\"\"Assert that inputs are compatible and formatted correctly!\"\"\"\n",
    "    \n",
    "    if output_path == '':\n",
    "        if not os.path.exists('outputs'):\n",
    "            os.makedirs('outputs')\n",
    "        output_path = './outputs'\n",
    "    globals()['output_path'] = output_path\n",
    "    \n",
    "    output_notes.append(f'Output files are located in: {output_path}')\n",
    "    input_list = values['input_list']\n",
    "    \n",
    "    # Check to make sure there's at least one desired output\n",
    "    assert len(values['input_list']) >= 1, 'Please select at least one output.'\n",
    "\n",
    "    if event == 'comparison':\n",
    "        # check that there are exactly 2 ds_ids selected\n",
    "        assert len(values['DS_IDs']) == 2, 'Incorrect number of DS_IDs selected.'\n",
    "\n",
    "        ds_selection = values['DS_IDs']\n",
    "        ds_selection.sort(reverse=True)\n",
    "        \n",
    "        first_ID, second_ID = ds_selection[0], ds_selection[1]\n",
    "        globals()['first_ID'] = first_ID\n",
    "        globals()['second_ID'] = second_ID\n",
    "        return\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outputs(event, created_dfs):\n",
    "    \"\"\"\n",
    "    Function that converts created dataframes into csv files.\n",
    "    \"\"\" \n",
    "    # comparison output\n",
    "    if event == 'comparison':\n",
    "        for df_name, df in created_dfs.items():\n",
    "            if 'diff' in df_name:\n",
    "                df.to_csv(f\"{output_path}/{df_name}_{first_ID}_minus_{second_ID}.csv\")\n",
    "            else:\n",
    "                df.to_csv(f\"{output_path}/{df_name}_{first_ID}_{second_ID}.csv\")\n",
    "        return\n",
    "    \n",
    "    # individual output\n",
    "    for df_name, df in created_dfs.items():\n",
    "        df.to_csv(f\"{output_path}/{df_name}_{individual_ID}.csv\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_dfs(event, first_ID, second_ID, input_list, output_notes):\n",
    "    \"\"\"\n",
    "    (Comparison Only)\n",
    "    Function that runs through desired outputs and creates dataframes based on selected desired outputs. This function\n",
    "    also saves any notes that need to be displayed to the user.\n",
    "    \"\"\"\n",
    "    # download data for each ds_id\n",
    "    first_ID_processed, first_ID_unprocessed, first_ID_features = download_DS_data(first_ID, jur_level)\n",
    "    second_ID_processed, second_ID_unprocessed, second_ID_features = download_DS_data(second_ID, jur_level)\n",
    "    \n",
    "    unshared_features = non_shared_features(first_ID_features, second_ID_features)\n",
    "    if len(unshared_features) > 0:\n",
    "        output_notes.append(f'Unshared features: {\", \".join(unshared_features)}')\n",
    "    else:\n",
    "        output_notes.append('All features are shared.')\n",
    "              \n",
    "    unshared_years = non_shared_years(first_ID_unprocessed, second_ID_unprocessed)\n",
    "    if len(unshared_years) > 0:\n",
    "        output_notes.append(f'Unshared years: {\", \".join(unshared_years)}')\n",
    "    else:\n",
    "        output_notes.append('All years are shared.')\n",
    "                            \n",
    "    for df in input_list:\n",
    "        if df[-4:] == 'diff':\n",
    "            geo_level, df_type = df.split('_')\n",
    "            output_notes.append(f'Differences in diff files were generated by calculating: {first_ID} values - {second_ID} values.')\n",
    "            output_notes.append(f'The {first_ID} and {second_ID} combined dataframe has also been outputted as {geo_level}_both_{first_ID}_{second_ID}.csv for reference.')\n",
    "                            \n",
    "#     if any(df[-4:] == 'diff' for df in input_list):\n",
    "        \n",
    "#         output_notes.append(f'Differences in diff files were generated by calculating: {first_ID} values - {second_ID} values.')\n",
    "#         output_notes.append(f'The {first_ID} and {second_ID} combined dataframe has also been outputted as both_{first_ID}_{second_ID}.csv.')\n",
    "    \n",
    "    output_notes.append(f'Base year for {first_ID} is {[item[-4:] for item in config[first_ID].keys()][0]}.')\n",
    "    output_notes.append(f'Base year for {second_ID} is {[item[-4:] for item in config[second_ID].keys()][0]}.')\n",
    "\n",
    "    created = {}\n",
    "    if 'mgra_both' in input_list:\n",
    "        mgra_both = concat_dfs_temp(first_ID_unprocessed, second_ID_unprocessed)\n",
    "        created['mgra_both'] = mgra_both\n",
    "    if 'cpa_both' in input_list:\n",
    "        cpa_both = cpa_aggregation(first_ID_unprocessed, second_ID_unprocessed, cpa_level)\n",
    "        created['cpa_both'] = cpa_both\n",
    "    if 'jur_both' in input_list: \n",
    "        jur_both = jur_aggregation(first_ID_unprocessed, second_ID_unprocessed, jur_level)\n",
    "        created['jur_both'] = jur_both\n",
    "    if 'region_both' in input_list:\n",
    "        region_both = region_aggregation(first_ID_unprocessed, second_ID_unprocessed)\n",
    "        created['region_both'] = region_both\n",
    "    if 'mgra_diff' in input_list:\n",
    "        if 'mgra_both' not in input_list:\n",
    "            mgra_both = concat_dfs_temp(first_ID_unprocessed, second_ID_unprocessed)\n",
    "        mgra_diff = create_diff_temp(first_ID_features, second_ID_features, mgra_both)\n",
    "        created['mgra_diff'] = mgra_diff\n",
    "        created['mgra_both'] = mgra_both\n",
    "    if 'cpa_diff' in input_list:\n",
    "        if 'cpa_both' not in input_list:\n",
    "            cpa_both = cpa_aggregation(first_ID_unprocessed, second_ID_unprocessed, cpa_level)\n",
    "        cpa_diff = create_diff(first_ID_features, second_ID_features, cpa_both)                            \n",
    "        created['cpa_diff'] = cpa_diff\n",
    "        created['cpa_both'] = cpa_both\n",
    "    if 'jur_diff' in input_list:\n",
    "        if 'jur_both' not in input_list:\n",
    "            jur_both = jur_aggregation(first_ID_unprocessed, second_ID_unprocessed, jur_level)\n",
    "        jur_diff = create_diff(first_ID_features, second_ID_features, jur_both)\n",
    "        created['jur_diff'] = jur_diff\n",
    "        created['jur_both'] = jur_both\n",
    "    if 'region_diff' in input_list:\n",
    "        if 'region_both' not in input_list:\n",
    "            region_both = region_aggregation(first_ID_unprocessed, second_ID_unprocessed)\n",
    "        region_diff = create_diff(first_ID_features, second_ID_features, region_both)\n",
    "        created['region_diff'] = region_diff\n",
    "        created['region_both'] = region_both\n",
    "\n",
    "    generate_outputs(event, created)\n",
    "                            \n",
    "    print(f\"{first_ID} & {second_ID} outputs generated successfully!\")\n",
    "                            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_window(output_path):\n",
    "    \"\"\"\n",
    "    Creates SimplePyGUI window that enables user to select multiple DS_ID's along with desired outputs. The window will\n",
    "    also have a console section where any output notes or errors will be displayed.\n",
    "    Returns click event as well as selected values (might remove return values since no purpose as of now).\n",
    "    \"\"\"\n",
    "    lb = sg.Listbox(values=comparison_selection_list, select_mode='multiple', size=(30, len(comparison_selection_list)+1), key='input_list')\n",
    "    \n",
    "    def select_all():\n",
    "        lb.set_value(comparison_selection_list)\n",
    "        return\n",
    "    def deselect_all():\n",
    "        lb.set_value([])\n",
    "        return\n",
    "    \n",
    "    layout_comparison = [\n",
    "        [sg.Button('Back', key='Back')],\n",
    "        [sg.Text('Please Select 2 DS_IDs')],\n",
    "        [sg.Listbox(values=(list(config.keys())[:-1]), select_mode='multiple', size=(30, len(config.keys())), key='DS_IDs')],\n",
    "        [sg.Text('Please Select Desired Outputs')],\n",
    "        [[sg.Button('Select All', target='input_list', key='select_all'), sg.Button('Clear All', target='input_list', key='clear_all')], lb],\n",
    "        [sg.Submit(key='comparison'), sg.Button('Cancel/Close', key='Cancel')],\n",
    "        [sg.Output(size=(100,20), key='output')]\n",
    "    ]\n",
    "    \n",
    "    window = sg.Window('Comparison window', layout_comparison, element_justification='c')\n",
    "    \n",
    "    while True: # Event Loop\n",
    "        event, values = window.Read()\n",
    "        if event in (None, 'Cancel', 'Back'):\n",
    "            break\n",
    "        if event == 'select_all':\n",
    "            select_all()\n",
    "        if event == 'clear_all':\n",
    "            deselect_all()\n",
    "        if event == 'comparison':\n",
    "            try:\n",
    "                output_notes = []\n",
    "                assert_inputs(event, values, output_path, output_notes)\n",
    "                print('Creating dataframes...')\n",
    "                create_comparison_dfs(event, first_ID, second_ID, values['input_list'], output_notes)\n",
    "                print()\n",
    "                print('\\n'.join(output_notes))\n",
    "                print()\n",
    "            except FileNotFoundError as f:\n",
    "                print('Please connect to the VPN and mount the T drive. If connected, please check YML file datapaths.')\n",
    "            except Exception as e:\n",
    "                print(traceback.format_exc())\n",
    "            \n",
    "    window.Close()\n",
    "    window['output'].__del__()\n",
    "    \n",
    "    if event == 'Back':\n",
    "        initiate_window()\n",
    "    \n",
    "    return event, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_individual_dfs(event, individual_IDs, input_list, output_notes):\n",
    "    \"\"\"\n",
    "    (Individual Only)\n",
    "    Function that runs through desired outputs and creates dataframes based on selected desired outputs. This function\n",
    "    also saves any notes that need to be displayed to the user.\n",
    "    \"\"\"\n",
    "    \n",
    "    for individual_ID in individual_IDs:\n",
    "        \n",
    "        globals()['individual_ID'] = individual_ID\n",
    "        \n",
    "        # download data for the ds_id\n",
    "        individual_ID_processed, individual_ID_unprocessed, individual_ID_features = download_DS_data(individual_ID, jur_level)\n",
    "\n",
    "        output_notes.append(f'Base year for {individual_ID} is {[item[-4:] for item in config[individual_ID].keys()][0]}.')\n",
    "\n",
    "        created = {}\n",
    "        if 'mgra_ind' in input_list:\n",
    "            mgra_ind = individual_ID_unprocessed.groupby(['mgra', 'year']).sum()\n",
    "            created['mgra_ind'] = mgra_ind\n",
    "        if 'cpa_ind' in input_list:\n",
    "            cpa_ind = cpa_aggregation_ind(individual_ID_unprocessed, cpa_level)\n",
    "            created['cpa_ind'] = cpa_ind\n",
    "        if 'jur_ind' in input_list:\n",
    "            jur_ind = jur_aggregation_ind(individual_ID_unprocessed, jur_level)\n",
    "            created['jur_ind'] = jur_ind\n",
    "        if 'region_ind' in input_list:\n",
    "            region_ind = region_aggregation_ind(individual_ID_unprocessed)\n",
    "            created['region_ind'] = region_ind\n",
    "            \n",
    "        generate_outputs(event, created)\n",
    "        print(f\"{individual_ID} {', '.join(created.keys())} outputs generated successfully!\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_window(output_path):\n",
    "    \"\"\"\n",
    "    Creates SimplePyGUI window that enables user to select a single DS_ID along with desired outputs. The window will\n",
    "    also have a console section where any output notes or errors will be displayed.\n",
    "    Returns click event as well as selected values (might remove return values since no purpose as of now).\n",
    "    \"\"\"\n",
    "    lb_options = sg.Listbox(values=individual_selection_list, select_mode='multiple', size=(30, len(individual_selection_list)+1), key='input_list')\n",
    "    lb_ds = sg.Listbox(values=(list(config.keys())[:-1]), select_mode='multiple', size=(30, len(config.keys())), key='individual_ID')\n",
    "    \n",
    "    def select_all_options():\n",
    "        lb_options.set_value(individual_selection_list)\n",
    "        return\n",
    "    def deselect_all_options():\n",
    "        lb_options.set_value([])\n",
    "        return\n",
    "    \n",
    "    def select_all_ds():\n",
    "        lb_ds.set_value(list(config.keys())[:-1])\n",
    "        return\n",
    "    def deselect_all_ds():\n",
    "        lb_ds.set_value([])\n",
    "        return\n",
    "        \n",
    "    layout_individual = [\n",
    "        [sg.Button('Back', key='Back')],\n",
    "        [sg.Text('Please Select DS_ID(s)')],\n",
    "        [[sg.Button('Select All', target='individual_ID', key='select_all_ds'), sg.Button('Clear All', target='individual_ID', key='clear_all_ds')], lb_ds],\n",
    "        [sg.Text('Please Select Desired Outputs')],\n",
    "        [[sg.Button('Select All', target='input_list', key='select_all'), sg.Button('Clear All', target='input_list', key='clear_all')], lb_options],\n",
    "        [sg.Submit(key='individual'), sg.Button('Cancel/Close', key='Cancel')],\n",
    "        [sg.Output(size=(100,20), key='output')]\n",
    "    ]\n",
    "    \n",
    "    window = sg.Window('Individual window', layout_individual, element_justification='c')\n",
    "    \n",
    "    while True: # Event Loop\n",
    "        event, values = window.Read()\n",
    "        if event in (None, 'Cancel', 'Back'):\n",
    "            break\n",
    "        if event == 'select_all':\n",
    "            select_all_options()\n",
    "        if event == 'clear_all':\n",
    "            deselect_all_options()\n",
    "        if event == 'select_all_ds':\n",
    "            select_all_ds()\n",
    "        if event == 'clear_all_ds':\n",
    "            deselect_all_ds()\n",
    "        if event == 'individual':\n",
    "            try:\n",
    "                output_notes = []\n",
    "                assert_inputs(event, values, output_path, output_notes)\n",
    "                print('Creating dataframes...')\n",
    "                create_individual_dfs(event, values['individual_ID'], values['input_list'], output_notes)\n",
    "                print()\n",
    "                print('\\n'.join(output_notes))\n",
    "                print()\n",
    "            except FileNotFoundError as f:\n",
    "                print('Please connect to the VPN and mount the T drive. If connected, please check YML file datapaths.')\n",
    "            except Exception as e:\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "    window.Close()\n",
    "    window['output'].__del__()\n",
    "    \n",
    "    if event == 'Back':\n",
    "        initiate_window()\n",
    "    \n",
    "    return event, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_window():\n",
    "    \"\"\"\n",
    "    Function that initiates the window flow process starting with the base window. Helps coordinate transfer from base\n",
    "    window to either comparison or individual window based on click event returned from base window.\n",
    "    \"\"\"\n",
    "    sg.theme('SandyBeach')\n",
    "    \n",
    "    # conn needs to be in function because conn can expire and need to be re-run\n",
    "    conn = pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};'\n",
    "                      'Server=DDAMWSQL16.sandag.org;'\n",
    "                      'Database=demographic_warehouse;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "    \n",
    "    event, values = base_window()\n",
    "    output_path = values['output-path']\n",
    "    while True:\n",
    "        if event in [None, 'Cancel']:\n",
    "            return\n",
    "        if event == 'comparison-select':\n",
    "            event, values = comparison_window(output_path)\n",
    "            return\n",
    "        if event == 'individual-select':\n",
    "            event, values = individual_window(output_path)\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO List:\n",
    "- Figure out what other output notes we need (especially for individual comparisons)\n",
    "- consider csv outputs as inputs to power bi\n",
    "\n",
    "\n",
    "- add mgra_id grouping for series 14 ds_ids (do we have csv files for series 14 we can test out? I think the ones we have been using are series 13.)\n",
    "- use outer join for the comparisons (**done but need to check if it works as intended**)\n",
    "- adjust sql query to be any series (currently queries only series 14 i think)\n",
    "\n",
    "series 14 mgra to jurisdiction: even if mgra falls into multiple juris, there are scenarios that one juridiction would report 0 to account for duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEAS to consider:\n",
    "\n",
    "- should we rename columns based on sql tables (for clarity on column meanings)? **Already done for housing cols but maybe there's more we can do**\n",
    "- generate outputs to different folders for comparison or individual?\n",
    "- should we order the DS_ID's in the selection list?\n",
    "- how are mgra csv files released? If there's a convention for folder file paths, maybe we could automate selection of new filepaths instead of using the yml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d60123f2ed8b63279fba5ddbefdeca593323e286d3975f7130d49323a9310301"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
