{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022-54 OPIS Fuel Price Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Plan: https://sandag.sharepoint.com/qaqc/_layouts/15/Doc.aspx?sourcedoc={c0fd7f23-7faa-4f2e-b7fb-79e477e131a6}&action=edit&wd=target%282022-41.one%7C5c70225e-b636-4090-a946-7545acf4abc3%2FTest%20Plan%7C683def87-dc70-4863-8cad-59d9ef21bd00%2F%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sql\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ddam = sql.create_engine('mssql+pymssql://DDAMWSQL16/dpoe_stage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_raw_data(user):\n",
    "    \"\"\"\n",
    "    Download the two raw data files. Note that copies of these files were put into SharePoint.\n",
    "\n",
    "    :param user:    The user downloading the data from SharePoint. This is mostly here so that it\n",
    "                    is easy for anyone to run the code\n",
    "    :returns:       Tuple containing two dataframes. In order, data contained comes from the files:\n",
    "                    \"Copy of San Diego Association of Governments.xlsx\"\n",
    "                    \"Copy of SanDiegoCountyJune2019\"\n",
    "    \"\"\"\n",
    "    # The folder where raw data is stored\n",
    "    base_url = Path(f\"C:/Users/{user}/San Diego Association of Governments/SANDAG QA QC - Documents/Service Requests/2022/2022-54 OPIS Fuel Price Data QC/data/\")\n",
    "\n",
    "    # The two raw data files we are getting\n",
    "    raw_files = [\n",
    "        Path(\"Copy of San Diego Association of Governments.xlsx\"),\n",
    "        Path(\"Copy of SanDiegoCountyJune2019.xlsx\"),\n",
    "    ]\n",
    "\n",
    "    # Get the two raw data files\n",
    "    # Note the different behaviors depending on file extension\n",
    "    raw_data = []\n",
    "    for file in raw_files:\n",
    "        raw_data.append(pd.read_excel(base_url / file))\n",
    "\n",
    "    # Return the two raw data files in tuple format\n",
    "    return tuple(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_SQL_data(connection):\n",
    "    \"\"\"\n",
    "    Download the contents of the two SQL tables joined together into one table\n",
    "\n",
    "    :param connection:  sqlalchemy connection to DDAMWSQL16/dpoe_stage \n",
    "    :returns:           One dataframe containing the combined contents of the two tables:\n",
    "                        [dpoe_stage].[fuel_price_opis].[price_fact] \n",
    "                        [dpoe_stage].[fuel_price_opis].[date_dim]\n",
    "    \"\"\"\n",
    "    # The tables are rather small, so there is no issue in just downloading both tables and holding\n",
    "    # them in memory\n",
    "    price_fact = pd.read_sql_query(\"\"\"\n",
    "        SELECT * FROM [dpoe_stage].[fuel_price_opis].[price_fact]\n",
    "        \"\"\", con=connection)\n",
    "    date_dim = pd.read_sql_query(\"\"\"\n",
    "        SELECT * FROM [dpoe_stage].[fuel_price_opis].[date_dim]\n",
    "        \"\"\", con=connection)\n",
    "\n",
    "    # Combine the tables along the \"date_id\" field of price_fact\n",
    "    combined = price_fact.merge(date_dim, on=\"date_id\")\n",
    "\n",
    "    # Make sure the \"date_code\" column is datetime\n",
    "    combined[\"date_code\"] = pd.to_datetime(combined[\"date_code\"]).dt.date\n",
    "\n",
    "    # Return the combined SQL tables (yes I know I could have done it in SQL but I perfer doing it\n",
    "    # in python)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw data and sql data\n",
    "retail_7_2019_to_4_2022, region_6_2019 = download_raw_data(\"eli\")\n",
    "source_data = download_SQL_data(ddam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests on the file: \"R:/DPOE/Fuel Price/OPIS/2021/Source/Copy of San Diego Association of Governments.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect years/months were loaded from SQL\n",
      "Dates in csv but not in SQL:     [Timestamp('2019-07-01 00:00:00')]\n",
      "Dates in SQL but not in csv:     []\n"
     ]
    }
   ],
   "source": [
    "# Check that the correct years and months were loaded (August 2020 to April 2022 (see email))\n",
    "\n",
    "# The correct years and months. Note \"MS\" is a frequency of every month start\n",
    "correct_date_range = pd.date_range(start=\"2020-08-01\", end=\"2022-04-01\", freq=\"MS\")\n",
    "\n",
    "# The actual loaded years and months\n",
    "csv_date_range = retail_7_2019_to_4_2022[\"Start Date\"].value_counts().index\n",
    "\n",
    "# Run test\n",
    "try:\n",
    "    correct_date_range == csv_date_range\n",
    "    print(\"Correct years/months were loaded from SQL\")\n",
    "except ValueError:\n",
    "    print(\"Incorrect years/months were loaded from SQL\")\n",
    "    print(f\"{'Dates in csv but not in SQL:': <32}\", list(csv_date_range.difference(correct_date_range)))\n",
    "    print(f\"{'Dates in SQL but not in csv:': <32}\", list(correct_date_range.difference(csv_date_range)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct number of rows were loaded from SQL\n"
     ]
    }
   ],
   "source": [
    "# Check the correct number of rows were loaded\n",
    "\n",
    "# Since csv data contains extra rows (as a result of the extra dates), remove them first\n",
    "csv_data = retail_7_2019_to_4_2022[\n",
    "    ~retail_7_2019_to_4_2022[\"Start Date\"].isin(csv_date_range.difference(correct_date_range))\n",
    "]\n",
    "\n",
    "# SQL data also contains extra rows, remove them first\n",
    "sql_data = source_data.copy(deep=True)\n",
    "sql_data = sql_data[pd.to_datetime(sql_data[\"date_code\"]).isin(correct_date_range)]\n",
    "\n",
    "# Test if the number of rows are the same\n",
    "test_result = csv_data.shape[0] == sql_data.shape[0]\n",
    "if(test_result):\n",
    "    print(\"Correct number of rows were loaded from SQL\")\n",
    "else:\n",
    "    print(\"Incorrect number of rows were loaded from SQL\")\n",
    "    print(f\"{'Rows in csv file:': <20}\", csv_data.shape[0])\n",
    "    print(f\"{'Rows in SQL:': <20}\", sql_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct number of rows for each date were loaded from SQL\n"
     ]
    }
   ],
   "source": [
    "# Check each date has the correct number of rows associated with it\n",
    "# Just use the same dfs as before, with the proper date filters already done\n",
    "\n",
    "# Test if the number of rows are the same\n",
    "test_result = (csv_data[\"Start Date\"].value_counts() == sql_data[\"date_code\"].value_counts())\n",
    "if(test_result.sum() - test_result.shape[0] == 0):\n",
    "    print(\"Correct number of rows for each date were loaded from SQL\")\n",
    "else:\n",
    "    print(\"Incorrect number of rows for each date were loaded from SQL\")\n",
    "    print(\"Differences are as follows\")\n",
    "    print(csv_data.value_counts()[test_result])\n",
    "    print(sql_data.value_counts()[test_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect column names were loaded from SQL\n",
      "Columns in csv but not in SQL:   ['Freight Average', 'Margin Average', 'Net Average', 'Region Name', 'Retail Average', 'Retail Product Name', 'Start Date', 'Tax Average', 'Wholesale Average']\n",
      "Columns in SQL but not in csv:   ['date_code', 'date_id', 'dy', 'freight_avg', 'margin_avg', 'mnth', 'net_avg', 'opis_id', 'product', 'qtr', 'region', 'retail_avg', 'season', 'station_count', 'tax_avg', 'wholesale_avg', 'yr']\n"
     ]
    }
   ],
   "source": [
    "# Check that column names/dtypes match\n",
    "\n",
    "# The correct columns, which come from SQL. Note \"MS\" is a frequency of every month start\n",
    "correct_columns = source_data.columns\n",
    "\n",
    "# The actual columns, which come from CSV\n",
    "csv_columns = retail_7_2019_to_4_2022.columns\n",
    "\n",
    "# Test the column names match\n",
    "try:\n",
    "    correct_columns == csv_columns\n",
    "    print(\"Correct column names were loaded from SQL\")\n",
    "except ValueError:\n",
    "    print(\"Incorrect column names were loaded from SQL\")\n",
    "    print(f\"{'Columns in csv but not in SQL:': <32}\", list(csv_columns.difference(correct_columns)))\n",
    "    print(f\"{'Columns in SQL but not in csv:': <32}\", list(correct_columns.difference(csv_columns)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('SANDAG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41c20cbfa603768ded13150453b5681e701a59febb16590ec3ab380b595ec114"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
